# Guided exercise: Restore from backup

In this exercise you will intentionally delete an API and later on restore management and portal subsystems to get that API back.

> Make sure that you have completed backup exercise.

1. Save the current state of ManagementCluster and PortalCluster CRs.
```
[root@think ~]# mkdir cr_backup
[root@think ~]# cd cr_backup/
[root@think cr_backup]# k get mgmt management -o yaml > management_cr.yaml
[root@think cr_backup]# k get ptl portal -o yaml > portal_cr.yaml

```

2. Edit resulting files so that:
   - `metadata` section has only `name` and `namespace` fields
   - `status` section is removed

3. Delete the current ManagementCluster and Portal deployments:
```
[root@think cr_backup]# k delete mgmt management
managementcluster.management.apiconnect.ibm.com "management" deleted
[root@think cr_backup]# k delete ptl portal
portalcluster.portal.apiconnect.ibm.com "portal" deleted
```
Wait until all management and portal pods are deleted.
```
[root@think cr_backup]# k get pod
[root@think cr_backup]# k get pod
NAME                                                     READY   STATUS        RESTARTS   AGE
analytics-cj-retention-1646962200-hjgz4                  0/1     Completed     0          13h
analytics-cj-rollover-1647009900-9p8s2                   0/1     Completed     0          2m54s
analytics-client-7764d79966-2ncv6                        1/1     Running       0          2d
analytics-ingestion-7d9d659666-xbj85                     1/1     Running       0          2d
analytics-mtls-gw-dfb7f8754-bx22h                        1/1     Running       0          2d
analytics-storage-coord-59f5f7df9b-d5cp2                 1/1     Running       0          2d
analytics-storage-data-0                                 1/1     Running       0          2d
analytics-storage-master-0                               1/1     Running       0          2d
datapower-operator-c69586975-mmqxr                       1/1     Running       4          308d
datapower-operator-conversion-webhook-5d65995b54-gzjcs   1/1     Running       6          308d
gwv6-0                                                   1/1     Running       0          2d
hostpath-provisioner-5dc69c8d77-hsrmz                    1/1     Running       6          308d
ibm-apiconnect-8f6cc69dc-lh97q                           1/1     Running       15         308d
tiller-deploy-6bfffc8ff-tnx2l                            1/1     Running       6          308d
```

Delete PVCs for both portal and management subsystems:
```
[root@think cr_backup]# for pvc in $(k get pvc -o name | egrep 'portal|management'); do k delete $pvc; done
persistentvolumeclaim "admin-portal-ad7d1c8d-www-0" deleted
persistentvolumeclaim "backup-portal-ad7d1c8d-www-0" deleted
persistentvolumeclaim "db-portal-ad7d1c8d-db-0" deleted
persistentvolumeclaim "dblogs-portal-ad7d1c8d-db-0" deleted
persistentvolumeclaim "management-93aee87d-postgres" deleted
persistentvolumeclaim "management-93aee87d-postgres-pgbr-repo" deleted
persistentvolumeclaim "management-93aee87d-postgres-wal" deleted
persistentvolumeclaim "web-portal-ad7d1c8d-www-0" deleted
```

Delete PVs that are no longer used:
```
[root@think cr_backup]# for pv in $(kubectl get pv | grep Released | awk '{print $1}'); do kubectl delete pv $pv; done
persistentvolume "pvc-25063b9b-3736-48c4-9708-f0e6b32ce8ca" deleted
persistentvolume "pvc-27441e38-d410-48e9-8308-cee3ac3c2a89" deleted
persistentvolume "pvc-4e6aac3f-8de8-4bcf-8987-1e51103dc6c2" deleted
persistentvolume "pvc-694e8380-8235-4220-9ea8-037de807cfe0" deleted
persistentvolume "pvc-97aa1f2f-7f45-446c-9db0-fa0f4b541615" deleted
persistentvolume "pvc-ad86b610-b09b-4735-935b-9afee567d8a8" deleted
persistentvolume "pvc-b5c2c0dd-fc13-43ca-97f3-e01195ade001" deleted
persistentvolume "pvc-c6eb62be-61b1-4b47-9b4d-93051cf3fd6d" deleted
persistentvolume "pvc-d1dc6389-c8d4-4d93-a19e-5ed9e51b3e72" deleted
```

You have now completely removed Management and Portal Subsystems 

## Restore Management and Portal subsystems

1. Create Management and Portal CRs from backed up definitions:
```
[root@think cr_backup]# k apply -f management_cr.yaml 
managementcluster.management.apiconnect.ibm.com/management created
[root@think cr_backup]# k apply -f portal_cr.yaml 
portalcluster.portal.apiconnect.ibm.com/portal created
```

2. Wait until both subsystems are brought back and are running:
```
[root@think cr_backup]# k get mgmt
NAME         READY   STATUS    VERSION        RECONCILED VERSION       AGE
management   16/16   Running   10.0.1.2-eus   10.0.1.2-ifix2-100-eus   7m25s
[root@think cr_backup]# k get ptl
NAME     READY   STATUS    VERSION        RECONCILED VERSION       AGE
portal   4/4     Running   10.0.1.2-eus   10.0.1.2-ifix2-100-eus   7m28s
[root@think cr_backup]# k get mgmtb
NAME                  STATUS   ID                 CLUSTER      TYPE   CR TYPE   AGE
management-6119c9b2   Ready    20220311-135320F   management   full   record    70s
management-6f13c7aa   Ready    20220311-143810F   management   full   record    70s
management-babb6c08   Ready    20220311-145609F   management   full   record    70s
[root@think cr_backup]# k get portalbackup
NAME               ID                STATUS   TYPE     CR TYPE   AGE   COMMENT
portal-bup-sctwm   20220311.143744   Ready    system   record    98s
portal-bup-szxrd   20220311.141212   Ready    system   record    98s
```
Note down the backups that you would like to restore. In this case it's going to be:
> management-babb6c08
> portal-bup-szxrd

3. Copy over sample restore custom resource definitions:
```
[root@think cr_backup]# cp ../distr/helpers/mgmtrestore_cr.yaml .
[root@think cr_backup]# cp ../distr/helpers/portalrestore_cr.yaml .
```

4. Edit `mgmtrestore_cr.yaml` files, so that you specify which backup to restore:
```yaml
apiVersion: management.apiconnect.ibm.com/v1beta1
kind: ManagementRestore
metadata:
  generateName: management-
spec:
  backupName: management-babb6c08babb6c08

```

5. Edit `portalrestore_cr.yaml` files, so that you specify which backup to restore:
```yaml
apiVersion: portal.apiconnect.ibm.com/v1beta1
kind: PortalRestore
metadata:
  generateName: portal-restore-
spec:
  type: all
  dryRun: false
  timestamp: now
```
6. Run restore of management subsystem:
```
[root@think cr_backup]# k create -f mgmtrestore_cr.yaml 
managementrestore.management.apiconnect.ibm.com/management-nswt2 created
```
You can monitor progress by running command:
```
[root@think cr_backup]# k get mgmtr
NAME               STATUS              MESSAGE                                                             BACKUP                CLUSTER      PITR   AGE
management-nswt2   RestoreInProgress   Waiting on management services to disable before starting restore   management-babb6c08   management          3s
[root@think cr_backup]# k get mgmtr
NAME               STATUS              MESSAGE                                                                             	BACKUP                CLUST
ER      PITR   AGE
management-nswt2   RestoreSuccessful   The job triggering Dynamic Reregistration and Reconfiguration after restore is Running   management-babb6c08   manag
ement          5m
[root@think cr_backup]# k get mgmtr
NAME               STATUS     MESSAGE                                            BACKUP                CLUSTER      PITR   AGE
management-nswt2   Complete   Restore process completed (DB Restore + DRR job)   management-babb6c08   management          5m21s

```

7. Run restore of portal subsystem:
```
[root@think cr_backup]# k create -f portalrestore_cr.yaml 
portalrestore.portal.apiconnect.ibm.com/portal-restore-6m8q4 created
```
You can monitor progress by running command:
```
[root@think cr_backup]# k get portalrestore
NAME                   STATUS    AGE
portal-restore-cbpqh   Running   10s
[root@think cr_backup]# k get portalrestore
NAME                   STATUS   AGE
portal-restore-cbpqh   Ready    83s
```


8. Login to Cloud Manager and verify that all objects have been restored (Topology, Resources, Settings).
   
   Login to API Manager and verify that all objects have been restored.
   
   Login to Developer Portal and verify if your site has been restored.


